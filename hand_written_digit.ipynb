{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# activation function\n",
    "def relu(inputs):\n",
    "    return np.maximum(inputs, 0)\n",
    "\n",
    "# output probability distribution function\n",
    "def softmax(inputs):\n",
    "    exp = np.exp(inputs)\n",
    "    return exp/np.sum(exp, axis = 1, keepdims = True)\n",
    "\n",
    "# loss\n",
    "def cross_entropy(inputs, y):\n",
    "    indices = np.argmax(y, axis = 1).astype(int)\n",
    "    probability = inputs[np.arange(len(inputs)), indices] #inputs[0, indices]\n",
    "    log = np.log(probability)\n",
    "    loss = -1.0 * np.sum(log) / len(log)\n",
    "    return loss\n",
    "\n",
    "# L2 regularization\n",
    "def L2_regularization(la, weight1, weight2):\n",
    "    weight1_loss = 0.5 * la * np.sum(weight1 * weight1)\n",
    "    weight2_loss = 0.5 * la * np.sum(weight2 * weight2)\n",
    "    return weight1_loss + weight2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "\n",
    "    def __init__(self, \n",
    "                 num_nodes_in_layers, \n",
    "                 batch_size,\n",
    "                 num_epochs,\n",
    "                 learning_rate\n",
    "                 ):\n",
    "\n",
    "        self.num_nodes_in_layers = num_nodes_in_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # build the network\n",
    "        #         w1/b1    w2/b2   \n",
    "        #784(inputs) ---> 20 ---> 10(output)\n",
    "        #         x     z1  a1  z2  a2=y\n",
    "        self.weight1 = np.random.normal(0, 1, [self.num_nodes_in_layers[0], self.num_nodes_in_layers[1]])\n",
    "        self.bias1 = np.zeros((1, self.num_nodes_in_layers[1]))\n",
    "        self.weight2 = np.random.normal(0, 1, [self.num_nodes_in_layers[1], self.num_nodes_in_layers[2]])\n",
    "        self.bias2 = np.zeros((1, self.num_nodes_in_layers[2]))\n",
    "        self.loss = []\n",
    "\n",
    "    def train(self, inputs, labels):\n",
    "\n",
    "        for epoch in range(self.num_epochs): # training begin\n",
    "            iteration = 0\n",
    "            while iteration < len(inputs):\n",
    "\n",
    "                # batch input\n",
    "                inputs_batch = inputs[iteration:iteration+self.batch_size]\n",
    "                labels_batch = labels[iteration:iteration+self.batch_size]\n",
    "                \n",
    "                # forward pass\n",
    "                z1 = np.dot(inputs_batch, self.weight1) + self.bias1\n",
    "                a1 = relu(z1)\n",
    "                z2 = np.dot(a1, self.weight2) + self.bias2\n",
    "                y = softmax(z2)\n",
    "                \n",
    "                # calculate loss\n",
    "                loss = cross_entropy(y, labels_batch)\n",
    "                loss += L2_regularization(0.01, self.weight1, self.weight2)#lambda\n",
    "                self.loss.append(loss)\n",
    "\n",
    "                # backward pass\n",
    "                delta_y = (y - labels_batch) / y.shape[0]\n",
    "                delta_hidden_layer = np.dot(delta_y, self.weight2.T) \n",
    "                delta_hidden_layer[a1 <= 0] = 0 # derivatives of relu\n",
    "\n",
    "                # backpropagation\n",
    "                weight2_gradient = np.dot(a1.T, delta_y) # forward * backward\n",
    "                bias2_gradient = np.sum(delta_y, axis = 0, keepdims = True)\n",
    "            \n",
    "                weight1_gradient = np.dot(inputs_batch.T, delta_hidden_layer)\n",
    "                bias1_gradient = np.sum(delta_hidden_layer, axis = 0, keepdims = True)\n",
    "\n",
    "                # L2 regularization\n",
    "                weight2_gradient += 0.01 * self.weight2\n",
    "                weight1_gradient += 0.01 * self.weight1\n",
    "\n",
    "                # stochastic gradient descent\n",
    "                self.weight1 -= self.learning_rate * weight1_gradient #update weight and bias\n",
    "                self.bias1 -= self.learning_rate * bias1_gradient\n",
    "                self.weight2 -= self.learning_rate * weight2_gradient\n",
    "                self.bias2 -= self.learning_rate * bias2_gradient\n",
    "\n",
    "                iteration += self.batch_size\n",
    "                \n",
    "            print('=== Epoch: %d/%d\\tLoss: %.2f ===' %(epoch+1, self.num_epochs, loss))\n",
    "\n",
    "    def test(self, inputs, labels):\n",
    "        input_layer = np.dot(inputs, self.weight1)\n",
    "        hidden_layer = relu(input_layer + self.bias1)\n",
    "        scores = np.dot(hidden_layer, self.weight2) + self.bias2\n",
    "        probs = softmax(scores)\n",
    "        acc = float(np.sum(np.argmax(probs, 1) == labels)) / float(len(labels))\n",
    "        print('test acc %.2f' % (acc*100))\n",
    "        \n",
    "    def predict(self, input):\n",
    "        input_layer = np.dot(input, self.weight1)\n",
    "        hidden_layer = relu(input_layer + self.bias1)\n",
    "        scores = np.dot(hidden_layer, self.weight2) + self.bias2\n",
    "        pred = softmax(scores)\n",
    "        pred[np.where(pred==np.max(pred))] = 1\n",
    "        pred[np.where(pred!=np.max(pred))] = 0\n",
    "        return pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "=== Epoch: 1/5\tLoss: 25.44 ===\n",
      "=== Epoch: 2/5\tLoss: 8.08 ===\n",
      "=== Epoch: 3/5\tLoss: 2.82 ===\n",
      "=== Epoch: 4/5\tLoss: 1.34 ===\n",
      "=== Epoch: 5/5\tLoss: 0.89 ===\n",
      "Testing...\n",
      "test acc 91.77\n"
     ]
    }
   ],
   "source": [
    "import mnist\n",
    "\n",
    "\n",
    "# load data\n",
    "num_classes = 10\n",
    "train_images = mnist.train_images() #[60000, 28, 28]\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "# data processing\n",
    "X_train = train_images.reshape(train_images.shape[0], train_images.shape[1]*train_images.shape[2]).astype('float32') #flatten 28x28 to 784x1 vectors, [60000, 784]\n",
    "x_train = X_train / 255 #normalization\n",
    "y_train = np.eye(num_classes)[train_labels] #convert label to one-hot\n",
    "\n",
    "X_test = test_images.reshape(test_images.shape[0], test_images.shape[1]*test_images.shape[2]).astype('float32') #flatten 28x28 to 784x1 vectors, [60000, 784]\n",
    "x_test = X_test / 255 #normalization\n",
    "y_test = test_labels\n",
    "\n",
    "net = Network(\n",
    "                 num_nodes_in_layers = [784, 20, 10], \n",
    "                 batch_size = 1,\n",
    "                 num_epochs = 5,\n",
    "                 learning_rate = 0.001\n",
    "             )\n",
    "\n",
    "net.train(x_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Testing...\")\n",
    "net.test(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1 is : 7\n",
      "image 2 is : 2\n",
      "image 3 is : 1\n",
      "image 4 is : 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test_images = mnist.test_images()\n",
    "\n",
    "f, axarr = plt.subplots(2,2)\n",
    "axarr[0,0].imshow(test_images[0])\n",
    "axarr[0,1].imshow(test_images[1])\n",
    "axarr[1,0].imshow(test_images[2])\n",
    "axarr[1,1].imshow(test_images[3])\n",
    "\n",
    "for i in range(4):\n",
    "    test_image = test_images[i]/255.0\n",
    "    test_image = net.predict(test_image.reshape(784))\n",
    "    print('image %d is : %d' % (i+1, test_image.argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
